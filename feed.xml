<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yiruyang2025.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yiruyang2025.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-12T16:37:06+00:00</updated><id>https://yiruyang2025.github.io/feed.xml</id><title type="html">Yiru Yang</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Benchmarks for Speech Processing Post-training - 25</title><link href="https://yiruyang2025.github.io/blog/2025/Benchmarks-for-Speech-Processing-Post-training-25/" rel="alternate" type="text/html" title="Benchmarks for Speech Processing Post-training - 25"/><published>2025-05-04T00:00:00+00:00</published><updated>2025-05-04T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Benchmarks%20for%20Speech%20Processing%20Post-training%20-%2025</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Benchmarks-for-Speech-Processing-Post-training-25/"><![CDATA[<p>Welcome! <br/><br/></p> <h1 id="1introduction">1.Introduction<br/><br/></h1> <p>Let‚Äôs discuss the main benchmarking metrics used in current industry practice.<br/><br/><br/><br/></p> <h1 id="2-recent-benchmark-frameworks">2. Recent Benchmark Frameworks<br/><br/></h1> <p>pending<br/><br/></p> <h2 id="21-evaluation-metrics-for-text-to-speech-tts">2.1 Evaluation Metrics for Text-to-Speech (TTS)</h2> <p>| Year | Metric | Description | Typical Range | |‚Äî‚Äî|‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî|</p> <p><br/><br/><br/><br/></p> <h2 id="22-evaluation-metrics-for-voice-conversion-vc">2.2 Evaluation Metrics for Voice Conversion (VC)</h2> <p>| Year | Metric | Description | Typical Range | |‚Äî‚Äî|‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî|</p> <p><br/><br/><br/><br/></p> <h2 id="23-speech-to-speech-translation-s2st-benchmarks">2.3 Speech-to-Speech Translation (S2ST) Benchmarks</h2> <p>| Year | Metric | Description | Typical Range | |‚Äî‚Äî|‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî|</p> <p><br/><br/><br/><br/></p> <h1 id="3specific-evaluation-frameworks-for-disabled-users">3.Specific Evaluation Frameworks for Disabled Users<br/><br/><br/><br/></h1>]]></content><author><name></name></author><category term="AI/ML"/><summary type="html"><![CDATA[üçã‚Äçüü©]]></summary></entry><entry><title type="html">Speech Processing - 25</title><link href="https://yiruyang2025.github.io/blog/2025/Speech-Processing-25/" rel="alternate" type="text/html" title="Speech Processing - 25"/><published>2025-05-04T00:00:00+00:00</published><updated>2025-05-04T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Speech%20Processing%20-%2025</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Speech-Processing-25/"><![CDATA[<p>Welcome!</p> <p>Let‚Äôs start with the Model Post-training for Hearing Assistance - An Coding Demo Example using xxxxx<br/><br/></p> <h1 id="0-some-background-knowledge">0. Some Background Knowledge<br/><br/></h1> <p><strong>0.1 Core Evolution of Voice Models</strong></p> <table> <thead> <tr> <th>Year</th> <th>Milestone</th> <th>Model / Paper</th> </tr> </thead> <tbody> <tr> <td>2014</td> <td>End-to-end ASR</td> <td>DeepSpeech (<a href="https://arxiv.org/abs/1412.5567">Hannun et al.</a>)</td> </tr> <tr> <td>2017</td> <td>Tacotron (neural TTS)</td> <td>Tacotron (<a href="https://arxiv.org/abs/1703.10135">Wang et al.</a>)</td> </tr> <tr> <td>2019</td> <td>Real-time voice synthesis</td> <td>FastSpeech (<a href="https://arxiv.org/abs/1905.09263">Ren et al.</a>)</td> </tr> <tr> <td>2020</td> <td>Self-supervised</td> <td>wav2vec 2.0 (<a href="https://arxiv.org/abs/2006.11477">Baevski et al.</a>)</td> </tr> <tr> <td>2022</td> <td>Multilingual speech models</td> <td>Whisper (<a href="https://github.com/openai/whisper">OpenAI, 2022</a>)</td> </tr> <tr> <td>2023</td> <td>Zero-shot voice cloning</td> <td>VALL-E (<a href="https://arxiv.org/abs/2301.02111">Microsoft, 2023</a>)</td> </tr> <tr> <td>2023‚Äì2024</td> <td>Diffusion-based TTS</td> <td>FastDiff (<a href="https://arxiv.org/abs/2305.10973">Huang et al.</a>)</td> </tr> <tr> <td>2024</td> <td>Multi-modal voice models</td> <td>AudioLM 2 (<a href="https://arxiv.org/abs/2402.05427">Borsos et al.</a>)</td> </tr> </tbody> </table> <p><br/><br/></p> <p><strong>0.2 Key Technical History</strong></p> <table> <thead> <tr> <th>Period</th> <th>Model Category</th> <th>Core Principle</th> </tr> </thead> <tbody> <tr> <td>2014‚Äì2017</td> <td>RNN - LSTM / GRU</td> <td>Sequence modeling, LSTM / GRU Solved vanishing gradient issues</td> </tr> <tr> <td>2018‚Äì2020</td> <td>Transformer / Conformer</td> <td>Self-Attention + CNN, Parallelizable computation for Efficiency</td> </tr> <tr> <td>2019‚Äì2022</td> <td>GAN-based Models</td> <td>TTS, Real-time audio Denoising for Hearing Aids</td> </tr> <tr> <td>2021‚ÄìPresent</td> <td>Diffusion Models</td> <td>Zero-shot / Few-shot</td> </tr> <tr> <td>Present</td> <td>SSL / Lightweight</td> <td>Self-supervised learning, Compression, Distillation</td> </tr> </tbody> </table> <p><br/><br/></p> <ul> <li><a href="https://www.sciencedirect.com/science/article/abs/pii/036402139090002E">RNN - 1990 Finding structure in time</a></li> </ul> <p><br/><br/></p> <ul> <li><a href="https://ieeexplore.ieee.org/abstract/document/6795963">LSTM - 1997 Long Short-Term Memory</a></li> </ul> <p><br/><br/></p> <ul> <li><a href="https://aclanthology.org/D14-1179/">GRU - 2014 Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation</a></li> </ul> <p><br/><br/></p> <ul> <li><a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Transformer - 2017 Attention Is All You Need</a></li> </ul> <p><br/><br/></p> <ul> <li><a href="https://arxiv.org/abs/1810.04805">BERT - 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>- Masked Language Modeling - MLM</li> </ul> <p><br/><br/></p> <ul> <li><a href="https://arxiv.org/abs/2005.08100">Conformer - 2020 Conformer: Convolution-augmented Transformer for Speech Recognition</a></li> </ul> <p><br/><br/></p> <ul> <li><a href="https://arxiv.org/abs/1406.2661">GAN - 2014 Generative Adversarial Networks</a></li> </ul> <p><br/><br/></p> <ul> <li> <p><strong>Diffusion Models</strong><br/></p> <ul> <li> <p><a href="https://arxiv.org/abs/1503.03585">2015 Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a><br/></p> </li> <li> <p><a href="https://arxiv.org/abs/2006.11239">2020 Denoising Diffusion Probabilistic Models</a><br/></p> </li> </ul> </li> </ul> <p><br/><br/></p> <ul> <li> <p><strong>SSL</strong><br/></p> <ul> <li><a href="https://arxiv.org/abs/1603.09246">2016 - Vision - Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</a><br/></li> <li><a href="https://arxiv.org/abs/1911.05722">2020 - Vision - Momentum Contrast for Unsupervised Visual Representation Learning</a><br/></li> <li><a href="https://arxiv.org/abs/1904.05862">2019 - Speech - wav2vec - Unsupervised Pre-training for Speech Recognition</a><br/></li> </ul> </li> </ul> <p><br/><br/></p> <p><br/><br/></p> <h1 id="1-some-sample-models-from-industry">1. Some Sample Models from Industry<br/><br/></h1> <p><strong>1.1 - Self-supervised</strong><br/></p> <p><br/></p> <p><br/><br/></p> <p><strong>1.2 - Zero-shot</strong><br/></p> <p><br/></p> <p><br/><br/></p> <p><strong>1.3 - Diffusion-based</strong><br/></p> <p><br/></p> <p><br/><br/></p> <p><strong>1.4 - Neural Audio Codec</strong><br/></p> <p><br/></p> <p><br/><br/></p> <p><em>**</em><i>1.5 - Multi-modal<i>** - will discuss in the future <br/></i></i></p> <p><br/><br/><br/><br/></p> <h1 id="2-model-training">2. Model Training<br/><br/></h1> <p><strong>2.1 Pre-training with text</strong><br/></p> <ul> <li><a href="https://arxiv.org/abs/2402.05755">Spirit LM: Interleaved Spoken and Written Language Model</a></li> <li><a href="https://openai.com/index/navigating-the-challenges-and-opportunities-of-synthetic-voices/">OpenAI - Navigating the Challenges and Opportunities of Synthetic Voices</a></li> <li><a href="https://arxiv.org/abs/2310.08715">Toward Joint Language Modeling for Speech Units and Text</a></li> <li><a href="https://arxiv.org/abs/2203.16502">Dialogue GSLM</a><br/><br/><br/></li> </ul> <p><strong>2.2 Post-Training</strong><br/></p> <ul> <li><strong>Pre-Train Style</strong> <ul> <li>üìç <strong>Distillation</strong></li> <li>SSL</li> <li>demo 1<br/><br/></li> </ul> <p><br/><br/></p> </li> <li><strong>Supervised-Fine-Tuning Style</strong> <ul> <li>üìç <strong>Adapter - LoRA / QLoRA</strong></li> <li>Prompt-tuning</li> <li>demo 2<br/><br/></li> </ul> <p><br/><br/></p> </li> <li><strong>Reinforcement-Learning Style</strong> <ul> <li>RLHF<br/><br/></li> </ul> <p><br/><br/></p> </li> <li>Others - Generative Enhancement Style <ul> <li>üìç <strong>DNN-GAN for Speech Denoising</strong></li> <li>demo 3<br/><br/></li> </ul> </li> </ul> <p><br/><br/><br/><br/></p> <h1 id="3-possible-improvements-to-the-foundation-models--during-fine-tuning">3. Possible Improvements to the Foundation Models / During Fine-Tuning<br/><br/></h1> <p><strong>3.1 Catastrophic Forgetting</strong></p> <ul> <li>2024 <a href="https://arxiv.org/abs/2401.05605">Scaling Laws for Forgetting When Fine-Tuning Large Language Models</a></li> <li>2023 <a href="https://arxiv.org/abs/2308.08747">An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning</a><br/><br/></li> </ul> <p><strong>Possible Solutions ü™®</strong></p> <ul> <li>2024 <a href="https://arxiv.org/abs/2405.09673">LoRA Learns Less and Forgets Less</a></li> <li>2018 <a href="https://arxiv.org/abs/1806.08730">The Natural Language Decathlon: Multitask Learning as Question Answering</a><br/><br/><br/><br/></li> </ul> <p><strong>3.2 Task Targeted Post-training will Degrade the model‚Äôs performance on other Tasks - e.g. Safety Alignment</strong></p> <ul> <li>Supervised-Fine-Tuning Style Post-training - 2024 <a href="https://arxiv.org/abs/2402.13669">Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning</a></li> <li>2023 <a href="https://arxiv.org/abs/2309.07875">Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions</a><br/><br/></li> </ul> <p><strong>Possible Solutions ü™®</strong></p> <p><br/><br/><br/><br/></p> <h1 id="4-recent-technical-advances---pay-attention-to-the--ones">4. Recent Technical Advances - pay attention to the üìç ones<br/><br/></h1> <ul> <li> <p><strong>2025 ‚Äì <a href="https://x.com/neuralink/status/1918005257252098197">Neuralink ‚Äì gets FDA nod for chip that will let speech impaired people speak, human trials soon</a></strong> <br/><br/> This includes those affected by ALS, stroke, spinal cord injury, cerebral palsy, multiple sclerosis, and other neurological conditions. <br/><br/></p> </li> <li> <p>üìç <strong>2024 ‚Äì <a href="https://openreview.net/forum?id=jOlO8t1xdx">Fast Timing-Conditioned Latent Audio Diffusion</a></strong> <br/><br/></p> </li> <li> <p>üìç <strong>2023 ‚Äì <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00618/118854">Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision</a></strong> <br/><br/> Proposes a novel approach to text-to-speech synthesis using minimal supervision while maintaining high fidelity, making TTS systems more accessible for low-resource settings.<br/><br/></p> </li> <li> <p>üìç <strong>2023 ‚Äì <a href="https://arxiv.org/abs/2306.00814">Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis</a></strong> <br/><br/></p> </li> <li> <p><strong>2023 ‚Äì <a href="https://about.fb.com/news/2023/06/introducing-voicebox-ai-for-speech-generation/">Voicebox: Versatile Generative Speech AI (Meta)</a></strong> <br/><br/> A generative model capable of text-to-speech, style transfer, noise removal, and speech editing using just 2 seconds of input audio.<br/><br/></p> </li> <li> <p>üìç <strong>2023 ‚Äì <a href="https://arxiv.org/abs/2301.02111">VALL-E: Zero-Shot Text-to-Speech via Neural Codec Language Modeling</a></strong> <br/><br/> Achieves personalized speech synthesis from a 3-second voice sample, preserving emotion and acoustic context in zero-shot TTS tasks.<br/><br/></p> </li> <li> <p><strong>2023 ‚Äì <a href="https://www.apple.com/newsroom/2023/05/apple-previews-live-speech-personal-voice-and-more-new-accessibility-features/">Apple Personal Voice &amp; Live Speech</a></strong> <br/><br/> Allows users to generate a personal synthetic voice using only 15 minutes of audio, aiding those at risk of speech loss due to ALS or other conditions.<br/><br/></p> </li> <li> <p><strong>2023 ‚Äì <a href="https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/">Meta Massively Multilingual Speech (MMS)</a></strong> <br/><br/> Open-source speech-to-text and text-to-speech models for 1,100+ languages, massively expanding multilingual accessibility in speech AI.<br/><br/></p> </li> <li> <p>üìç <strong>2022 ‚Äì <a href="https://openai.com/research/whisper">Whisper: Multilingual ASR via Large-Scale Weak Supervision</a></strong> <br/><br/> A general-purpose speech recognition system trained on 680,000 hours of audio, robust across accents, background noise, and multiple languages.<br/><br/></p> </li> <li> <p><strong>2021 ‚Äì <a href="https://www.apple.com/newsroom/2021/06/ios-15-brings-new-ways-to-stay-connected-and-powerful-features-that-help-users-focus-explore-and-do-more-with-on-device-intelligence/">Apple On-Device Speech Recognition for Siri</a></strong> <br/><br/> Introduced local processing of Siri speech recognition, enhancing privacy and enabling offline voice commands.<br/><br/></p> </li> <li> <p>üìç <strong>2020 ‚Äì <a href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: Self-Supervised Learning of Speech Representations</a></strong> <br/><br/> Demonstrated state-of-the-art ASR using very limited labeled data via self-supervised learning on large-scale unlabeled audio.<br/><br/></p> </li> <li> <p>üìç <strong>2020 ‚Äì <a href="https://arxiv.org/abs/2005.08100">Conformer: Convolution-augmented Transformer for ASR</a></strong> <br/><br/> Combined CNNs and Transformers for effective modeling of both local and global features in speech, improving ASR accuracy.<br/><br/></p> </li> <li> <p><strong>2019 ‚Äì <a href="https://blog.google/outreach-initiatives/accessibility/speech-accessibility-project/">Project Euphonia (Google)</a></strong> <br/><br/> Uses AI to improve ASR for users with atypical speech, such as those with ALS or other disorders, enhancing speech accessibility.<br/><br/></p> </li> <li> <p>üìç <strong>2016 ‚Äì <a href="https://arxiv.org/abs/1609.03499">WaveNet: A Generative Model for Raw Audio</a></strong> <br/><br/> Introduced deep generative modeling of raw audio, setting a new bar for natural-sounding speech synthesis.<br/><br/></p> </li> <li> <p>üìç <strong>2015 ‚Äì <a href="https://arxiv.org/abs/1512.02595">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a></strong> <br/><br/> Demonstrated that deep learning can perform ASR across languages and noisy conditions without hand-engineered features.<br/><br/></p> </li> <li> <p><strong>2014 ‚Äì <a href="https://arxiv.org/abs/1412.5567">Deep Speech: Scaling up End-to-End Speech Recognition</a></strong><br/> One of the first end-to-end deep learning systems for large-scale speech recognition, trained on hundreds of hours of data.<br/><br/><br/><br/></p> </li> </ul> <h1 id="5-products-for-disabled-people--hearing-aid-enhancement">5. Products for Disabled People / Hearing Aid Enhancement<br/><br/></h1> <p><strong>5.1 Key References</strong><br/><br/></p> <ul> <li> <p><a href="https://pubmed.ncbi.nlm.nih.gov/34670100/">2023 A scoping review of literature using speech recognition technologies by individuals with disabilities in multiple contexts</a></p> </li> <li> <p><a href="https://www.scienceopen.com/hosted-document?doi=10.57197%2FJDR-2023-0063">2024 ASR - Using Voice Technologies to Support Disabled People</a></p> </li> </ul> <p><br/></p> <p><strong>5.2 Aspects</strong><br/><br/></p> <ul> <li> <p>Model Adaptability<br/><br/></p> </li> <li> <p>Computational Efficiency<br/><br/></p> </li> <li> <p>Customization / Personalization<br/><br/></p> </li> </ul> <p><strong>5.3 Products</strong><br/><br/></p> <ul> <li> <p><strong>2024 Hearing Tracker</strong> - <a href="https://www.hearingtracker.com/resources/ai-in-hearing-aids-a-review-of-brands-and-models">Hearing Aids with Artificial Intelligence (AI): Review of Features, Capabilities and Models that Use AI and Machine Learning</a><br/><br/></p> </li> <li> <p><strong>2023 DNN</strong> - <a href="https://www.nature.com/articles/s41598-023-29871-8">Restoring speech intelligibility for hearing aid users with deep learning</a></p> </li> </ul> <p><br/><br/><br/></p> <h1 id="6-some-startups">6. Some Startups<br/><br/></h1> <ul> <li><strong><a href="https://www.audioshake.ai/">AudioShake</a></strong><br/> <ul> <li>Key Tech<br/> <ul> <li>Stem Separation - <strong>CNN / RNN</strong><br/></li> </ul> </li> <li>Markets <ul> <li>Music production, film and television post-production, podcast editing, game audio processing, user-generated content - UGC<br/></li> </ul> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p><strong><a href="https://elevenlabs.io/">ElevenLabs</a></strong> <br/></p> <ul> <li>Key Tech <ul> <li>TTS, Voice Cloning, Voice Conversion, STT - <strong>DNN</strong><br/></li> </ul> </li> <li>Markets <ul> <li>Audiobooks, podcast production, game dubbing, virtual assistants, educational content, film and television dubbing<br/></li> </ul> </li> <li><a href="https://github.com/elevenlabs/elevenlabs-python">Python SDK</a><br/></li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p><strong><a href="https://livekit.io/">LiveKit</a></strong> <br/></p> <ul> <li>Key Tech <ul> <li>Real-Time Communication Platform, Voice AI Agent Framework, Edge Infrastructure<br/></li> <li><strong>Transformer / DNN / VAD</strong><br/></li> </ul> </li> <li>Markets <ul> <li>Live video conferencing, voice chat, virtual events, online education, customer support<br/></li> </ul> </li> <li><a href="https://github.com/livekit/livekit">livekit</a><br/></li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p><strong><a href="https://www.realavatar.ai/">RealAvatar.ai</a></strong> <br/></p> <ul> <li>Key Tech <ul> <li>Multimodal AI Interaction, AI Avatars - <strong>DNN / Transformer</strong><br/></li> </ul> </li> <li>Markets <ul> <li>Education and training, customer service, virtual assistant, online consultation, content creation<br/></li> </ul> </li> </ul> </li> </ul> <p><br/><br/><br/><br/></p> <h1 id="7-some-terms-and-their-nature">7. Some Terms and their Nature<br/><br/></h1> <ul> <li> <p><strong>Attention</strong> - Vector of Importance Weights<br/></p> </li> <li> <p><strong>Encoder</strong> - Bidirectional RNN<br/></p> </li> <li> <p><strong>Additive Attention</strong> - <a href="https://arxiv.org/abs/1409.0473">2014 Neural Machine Translation by Jointly Learning to Align and Translate</a><br/></p> </li> <li> <p><strong>Dot-product Attention</strong> - <a href="https://arxiv.org/abs/1706.03762">2023 Attention Is All You Need</a><br/></p> </li> </ul> <h3 id="additive-attention">Additive Attention</h3> <p>Additive Attention computes the attention scores using a feed-forward neural network</p> \[e_i = \mathbf{v}^T \tanh(\mathbf{W}_1 \mathbf{q} + \mathbf{W}_2 \mathbf{k}_i)\] \[\alpha_i = \frac{\exp(e_i)}{\sum_j \exp(e_j)}\] \[\mathbf{c} = \sum_i \alpha_i \mathbf{v}_i\] <h3 id="dot-product-attention">Dot-Product Attention</h3> <p>Dot-Product Attention calculates the attention scores by taking the dot product of the query and key vectors</p> \[e_i = \mathbf{q}^T \mathbf{k}_i\] \[\alpha_i = \frac{\exp(e_i)}{\sum_j \exp(e_j)}\] \[\mathbf{c} = \sum_i \alpha_i \mathbf{v}_i\] <h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3> <p>To mitigate the issue of large dot product values in high-dimensional spaces, Scaled Dot-Product Attention scales the dot products</p> \[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V}\] <p><br/><br/></p> <ul> <li> <p><strong>Attention Layer</strong> - Parameterized by a simple feed-forward network<br/></p> </li> <li> <p><strong>Decoder</strong> - RNN with input from previous state + dynamic context vector<br/></p> </li> <li> <p><a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb"><strong>Tensor2Tensor Notebook</strong></a><br/></p> </li> <li> <p><strong>Self-Attention</strong> - <br/></p> </li> <li> <p><strong>Multi-head Self-attention</strong> - <br/></p> </li> <li> <p><strong>Activation Functions</strong> <br/></p> <ul> <li> <p><strong>Softmax</strong>:<br/> \(\alpha_i = \frac{\exp(e_i)}{\sum_j \exp(e_j)}\)<br/> Used to normalize attention scores into a probability distribution over keys<br/></p> </li> <li> <p><strong>Tanh</strong>:<br/> \(\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)<br/> Maps input to the range ([-1, 1]), commonly used in RNNs and attention scoring<br/></p> </li> <li> <p><strong>ReLU - Rectified Linear Unit</strong>:<br/> \(\text{ReLU}(x) = \max(0, x)\)<br/> Introduces sparsity and alleviates the vanishing gradient problem<br/></p> </li> <li> <p><strong>GELU - Gaussian Error Linear Unit</strong>:<br/> \(\text{GELU}(x) = x \cdot \Phi(x)\)<br/> where \(\Phi(x) = \frac{1}{2} \left[ 1 + \text{erf} \left( \frac{x}{\sqrt{2}} \right) \right]\) is the standard Gaussian cumulative distribution function - CDF GELU is smoother than ReLU and is widely used in Transformers<br/></p> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p><strong>Why need Positional Encodings</strong> - To give the model a sense of token order, since Transformers have no recurrence or convolution<br/></p> </li> <li> <p><strong>Why Adding Residual Connections</strong> - To Ease Gradient Flow and Improve Training Stability in Deep Networks<br/></p> </li> </ul> <p><br/><br/><br/></p> <h3 id="normalizations"><strong>Normalizations</strong><br/><br/></h3> <ul> <li> <p><strong>Layer Normalization - For Speech Processing - Transformer</strong> - To normalize inputs across features, speeding up convergence and Improving Generalization<br/></p> </li> <li> <p><strong>Batch Normalization - CNN / MLP</strong> - To stabilize learning by normalizing across the batch for each feature<br/></p> </li> <li> <p><strong>Instance Normalization</strong> - To normalize each sample per channel, useful for style transfer and Image Generation<br/></p> </li> <li> <p><strong>Group Normalization - Small Batch Image Generation</strong> - To divide channels into groups and normalize within each group<br/></p> </li> <li> <p><strong>RMSNorm - Lightweight Transformer</strong> - A simplified version of LayerNorm without centering the mean.<br/></p> </li> <li> <p><strong>Weight Normalization - RL / Sparse Network</strong> - Normalizes weight vectors instead of activations.<br/></p> </li> </ul> <p><br/><br/><br/></p> <h1 id="references">References<br/><br/></h1> <ul> <li> <p><a href="https://2025.ieeeicassp.org/">ICASSP - IEEE Intl. Conf. on Acoustics, Speech and Signal Processing</a><br/></p> </li> <li> <p><a href="https://www.interspeech2025.org/home">INTERSPEECH - Intl. Conf. on Spoken Language Processing</a><br/></p> </li> <li> <p><a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-audio-speech-and-language-processing">TASLP - IEEE/ACM Trans. on Audio, Speech, and Language Processing</a><br/></p> </li> </ul> <p><br/><br/><br/><br/></p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[‚õ∫Ô∏è]]></summary></entry><entry><title type="html">A Regularization Method for Causal Inference Identification - 26</title><link href="https://yiruyang2025.github.io/blog/2025/Economics-Story-Series-25/" rel="alternate" type="text/html" title="A Regularization Method for Causal Inference Identification - 26"/><published>2025-05-01T00:00:00+00:00</published><updated>2025-05-01T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Economics%20Story%20Series%20-%2025</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Economics-Story-Series-25/"><![CDATA[<p>Welcome to story time :) <br/><br/></p> <h1 id="topic-for-2025">Topic for 2025<br/><br/></h1> <h1 id="1-causal-inference-in-policy-design-x-economics---the-favourite-one">1. Causal Inference in Policy Design x Economics - The Favourite one<br/><br/></h1> <p>Causal identification in policy evaluation is a central challenge in economic research. Machine learning methods offer promising tools for denoising causal estimates, particularly in small-sample settings. The key here is to Reduce the Estimator Variance first. <br/><br/></p> <p><strong>TLDR - Key References</strong><br/></p> <ul> <li><a href="https://wol.iza.org/articles/machine-learning-for-causal-inference-in-economics/long">2025 IZA World of Labor - Machine learning for causal inference in economics</a><br/><br/></li> </ul> <p><strong>Possible Solutions ü™®</strong><br/></p> <ul> <li> <p><a href="https://grf-labs.github.io/grf/reference/causal_forest.html">Causal forest</a><br/></p> </li> <li> <p><a href="https://docs.doubleml.org/stable/guide/resampling.html#sample-splitting-cross-fitting-and-repeated-cross-fitting">Double/debiased machine learning - DML - by Sample-splitting, cross-fitting and repeated cross-fitting</a><br/></p> </li> <li> <p><strong>Bayesian Shrinkage</strong> - for Macro Panel - <a href="https://arxiv.org/abs/2504.05489">2025 Bayesian Shrinkage in High-Dimensional VAR Models: A Comparative Study</a><br/></p> </li> <li> <p><strong>Regularization</strong> - <a href="https://books.google.ch/books?hl=en&amp;lr=&amp;id=vNqPEAAAQBAJ&amp;oi=fnd&amp;pg=PA2&amp;dq=Structural+Reforms+and+Economic+Growth:+A+Machine+Learning+Approach&amp;ots=Qd7GZ_OtMf&amp;sig=ssZQ6t--rYGayW8nPml6WRfVKyo&amp;redir_esc=y#v=onepage&amp;q=Structural%20Reforms%20and%20Economic%20Growth%3A%20A%20Machine%20Learning%20Approach&amp;f=false">2022 IMF - Structural Reforms and Economic Growth: A Machine Learning Approach</a><br/></p> </li> <li> <p>Random Walks - <a href="https://www.imf.org/en/Publications/WP/Issues/2024/12/14/Reconciling-Random-Walks-and-Predictability-A-Dual-Component-Model-of-Exchange-Rate-Dynamics-559469">2024 IMF - Reconciling Random Walks and Predictability: A Dual- Component Model of Exchange Rate Dynamics</a><br/></p> </li> <li> <p><a href="https://www.elibrary.imf.org/view/journals/002/2024/071/article-A003-en.xml">PCA - Principal Component Analysis - 2024 IMF - Exchange Rate Dynamics: A Principal Components Analysis</a></p> <ul> <li><a href="https://www.elibrary.imf.org/view/journals/002/2024/071/article-A003-en.xml">2024 IMF - Exchange Rate Dynamics: A Principal Components Analysis</a></li> </ul> </li> </ul> <p><br/><br/></p> <p><strong>Key References</strong><br/><br/></p> <ul> <li> <p><a href="https://arxiv.org/abs/2403.14385">2024, Estimating Causal Effects with Double Machine Learning ‚Äì A Method Evaluation</a><br/><br/></p> </li> <li> <p><a href="https://arxiv.org/abs/2404.13356">2024, How do applied researchers use the Causal Forest? A methodological review of a method</a><br/><br/></p> </li> <li> <p><a href="https://books.google.ch/books?hl=en&amp;lr=&amp;id=TTHYEAAAQBAJ&amp;oi=fnd&amp;dq=Dining+and+Wining+During+the+Pandemic%3F+A+Quasi-Experiment+on+Tax+Cuts+and+Consumer+Spending&amp;ots=rmHpNy83Ob&amp;sig=3h07y-XrNnesJefX2Pf8FvkPQVI&amp;redir_esc=y">2023, IMF - Dining and Wining During the Pandemic? A Quasi-Experiment on Tax Cuts and Consumer Spending</a><br/><br/></p> </li> <li> <p><a href="https://link.springer.com/article/10.1186/s41937-023-00113-y">2023, Swiss Journal of Economics and Statistics - Causal Machine Learning and its use for public policy</a><br/><br/></p> </li> <li> <p><a href="https://onlinelibrary.wiley.com/doi/full/10.3982/ECTA18515?casa_token=STItJz1tJSIAAAAA%3Ajk68Kw29nujyb4bmBRn3Ji59ixhLbN1Vh3cB5TNRySXR-TO0nHLakCnm3-U_EUDFIfJ1uCwxTbKbtHc">2022, Econometrica - Automatic Debiased Machine Learning of Causal and Structural Effects</a><br/><br/></p> </li> <li> <p><a href="https://books.google.ch/books?hl=en&amp;lr=&amp;id=Ta0aEAAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=https://www.imf.org/-/media/Files/Publications/WP/2019/wpiea2019228-print-pdf.ashx&amp;ots=gKd_K9KoTO&amp;sig=v9UueJTOOxRLbzSKN7sLG16ylV4&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">2019, IMF - Machine Learning and Causality: The Impact of Financial Crises on Growth</a><br/><br/></p> </li> </ul> <p><br/><br/></p> <h1 id="topics-for-2026">Topics for 2026<br/><br/></h1> <h1 id="1-why-does-history-reduplicate-itself">1. Why does History Reduplicate Itself<br/><br/></h1> <p><br/><br/><br/><br/><br/><br/><br/><br/></p> <h1 id="2-models-matching">2. Models Matching<br/><br/></h1> <ul> <li>References<br/><br/> <ul> <li><a href="https://www.nature.com/articles/460685a">The economy needs agent-based modelling, 2009.</a><br/><br/></li> <li><a href="https://github.com/lu-group/pinn-macro-finance">Economics PINN: Deep learning for solving and estimating dynamic macro-finance models</a><br/><br/></li> <li><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5123878">Payne, Jonathan, Adam Rebei, and Yucheng Yang. 2025. Deep Learning for Search and Matching Models. February 1, 2025.</a><br/><br/></li> <li><a href="https://dl.acm.org/doi/abs/10.1145/3209978.3210181?casa_token=pq_eQ9XrOvsAAAAA:96fZTufb1_1bGgdg51D2tOeJWlvlHf8SYonh2ERMX5nCT3GaAoTGMDtqwg0f8Ls8ODbDSBpG5t-y">Xu, Jun, Xiangnan He, and Hang Li. 2020. Deep Learning for Matching in Search and Recommendation. Vol. XX, No. XX, pp. 1‚Äì193. DOI: XXX.</a><br/><br/><br/><br/></li> </ul> </li> </ul> <p>If you‚Äôre interested in the field of Academic Economics, pay attention to our <strong>Top 5 journals</strong>:<br/><br/></p> <ul> <li>1 <a href="https://www.aeaweb.org/journals/aer">AER - American Economic Review</a></li> <li>2 <a href="https://academic.oup.com/qje">QJE - Quarterly Journal of Economics</a></li> <li>3 <a href="https://www.journals.uchicago.edu/loi/jpe">JPE - Journal of Political Economy</a></li> <li>4 <a href="https://www.econometricsociety.org/publications/econometrica">Econometrica</a></li> <li>5 <a href="https://academic.oup.com/restud">Review of Economic Studies</a><br/><br/><br/><br/></li> </ul> <p><strong>For Working Papers</strong>:</p> <ul> <li>1 <a href="https://www.nber.org/papers?page=1&amp;perPage=50&amp;sortBy=public_date">NBER Working Papers</a></li> <li>2 <a href="https://www.aeaweb.org/journals/jel">Journal of Economic Literature (JEL)</a></li> <li>3 <a href="https://www.ssrn.com/index.cfm/en/">SSRN - Social Science Research Network</a></li> <li>4 <a href="https://cepr.org/voxeu">VoxEU.org</a></li> <li>5 <a href="https://www.nobelprize.org/prizes/economic-sciences/">Nobel Prize-winning Research</a></li> <li>6 <a href="https://www.aeaweb.org/about-aea/honors-awards/bates-clark">John Bates Clark Medal - AEA</a></li> <li>7 <a href="https://www.fbbva.es/en/">BBVA Frontiers of Knowledge Award ‚Äì Economics, Finance, and Management</a></li> <li>8 <a href="https://www.eeassoc.org/awards/yrjo-jahnsson-award">Yrj√∂ Jahnsson Award - EEA</a><br/><br/><br/></li> </ul> <p><strong>Others</strong>:</p> <ul> <li>1 <a href="https://afajof.org/">Journal of Finance</a></li> <li>2 <a href="https://www.jstor.org/journal/accountingreview">The Accounting Review</a></li> <li>3 <a href="https://www.imf.org/en/Publications/WP">IMF - Working papers</a></li> </ul> <p><br/><br/><br/></p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[‚õ∫Ô∏è]]></summary></entry><entry><title type="html">Pancake Time with Chess Puzzles</title><link href="https://yiruyang2025.github.io/blog/2025/Pancake-Time-with-Chess-Puzzles/" rel="alternate" type="text/html" title="Pancake Time with Chess Puzzles"/><published>2025-04-29T00:00:00+00:00</published><updated>2025-04-29T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Pancake%20Time%20with%20Chess%20Puzzles</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Pancake-Time-with-Chess-Puzzles/"><![CDATA[<p>Welcome to the Pancake Time :)</p> <p><br/><br/><br/><br/></p>]]></content><author><name></name></author><category term="Life"/><summary type="html"><![CDATA[ü•û]]></summary></entry><entry><title type="html">API Design - 25</title><link href="https://yiruyang2025.github.io/blog/2025/API-Design/" rel="alternate" type="text/html" title="API Design - 25"/><published>2025-04-28T00:00:00+00:00</published><updated>2025-04-28T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/API%20Design</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/API-Design/"><![CDATA[<p>Welcome!<br/><br/> Let me tell u how to design a your own API</p>]]></content><author><name></name></author><category term="AI/ML"/><summary type="html"><![CDATA[üîß]]></summary></entry><entry><title type="html">Lessons Learnt from parents and grandparents</title><link href="https://yiruyang2025.github.io/blog/2025/Lessons-Learnt-from-parents-and-grandparents/" rel="alternate" type="text/html" title="Lessons Learnt from parents and grandparents"/><published>2025-04-27T00:00:00+00:00</published><updated>2025-04-27T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Lessons%20Learnt%20from%20parents%20and%20grandparents</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Lessons-Learnt-from-parents-and-grandparents/"><![CDATA[<p>Welcome :)</p> <p>Here are my main takeaways:</p> <ul> <li>Always cherish those ‚ÄòReal‚Äô Moments in your life</li> <li>Nothing‚Äôs more important than our own families and good friends</li> <li>Beliefs are always important</li> <li>When you still have energy and passion, build your own bridge and Garden üåÅ</li> <li> <p>Always remember to Connect Your Own Dots</p> </li> <li> <p>For everything else, just BE PATIENT :)</p> </li> <li>‚ÄúThe prepared mind sooner or later finds things important and does it‚Äù - <a href="https://en.wikipedia.org/wiki/Richard_Hamming">Richard Hamming</a>, Thus, ‚Äútry to be well prepared at most times in your life‚Äù - my own the Best Grandpa</li> </ul> <p><img src="/assets/img/family.jpg" alt="pic1"/></p> <ul> <li>üçÄ</li> </ul>]]></content><author><name></name></author><category term="Life"/><summary type="html"><![CDATA[ü¶©]]></summary></entry><entry><title type="html">Why„ÄåDeep„ÄçStructures - 26</title><link href="https://yiruyang2025.github.io/blog/2025/Why-Deep-Structures-26/" rel="alternate" type="text/html" title="Why„ÄåDeep„ÄçStructures - 26"/><published>2025-04-27T00:00:00+00:00</published><updated>2025-04-27T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Why%E3%80%8CDeep%E3%80%8DStructures%20-%2026</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Why-Deep-Structures-26/"><![CDATA[<p>Welcome!<br/><br/></p> <p>Let‚Äôs take a look at the history of Deep Learning Models we‚Äôre using today.<br/><br/></p> <h1 id="1-key-tech">1. Key Tech<br/><br/></h1> <ul> <li><strong>RNN</strong> <ul> <li>When inputs are sequences<br/></li> <li><a href="https://ieeexplore.ieee.org/abstract/document/6795963">Hochreiter &amp; Schmidhuber 1997 - LSTM</a><br/></li> </ul> </li> </ul> <p><br/><br/></p> <ul> <li><strong>Transformer</strong> <ul> <li>When inputs are sequences<br/></li> <li>Self-attention + Parallel computation<br/></li> </ul> </li> </ul> <p><br/><br/></p> <ul> <li><strong>BERT</strong> <ul> <li>Bidirectional Encoder Representations from Transformers<br/></li> <li>using Masked language modeling<br/></li> <li><a href="https://aclanthology.org/N19-1423/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC">2019 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br/></li> </ul> </li> </ul> <p><br/><br/></p> <ul> <li><strong>Conformer</strong> <ul> <li>Convolution + Transformer = Conformer</li> <li>Combines local - CNN - and Global Self-attention Features</li> <li>Widely used in speech recognition tasks</li> <li><a href="https://arxiv.org/abs/2005.08100">2020 - Conformer: Convolution-augmented Transformer for Speech Recognition</a><br/></li> </ul> </li> </ul> <p><br/><br/></p> <ul> <li><strong>GAN</strong> <ul> <li>Generator vs Discriminator<br/></li> <li>Generates Images, Audio<br/></li> <li>Popular in TTS, audio enhancement, and image generation<br/></li> <li><a href="https://proceedings.neurips.cc/paper_files/paper/2014/hash/f033ed80deb0234979a61f95710dbe25-Abstract.html">2014 - Generative Adversarial Nets</a><br/></li> </ul> </li> </ul> <p><br/><br/></p> <ul> <li><strong>üìç Diffusion Based</strong> <ul> <li>Gradual denoising process to generate samples from noise<br/></li> <li>Currently SoTA in image and speech generation<br/></li> <li>Training is stable, generation is slow<br/></li> <li><a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">2020 - Denoising Diffusion Probabilistic Models</a><br/></li> </ul> </li> </ul> <p><br/><br/></p> <ul> <li><strong>üìç SSL</strong> <ul> <li>Learns from unlabeled data by solving pretext tasks<br/></li> <li>Strong performance in low-resource and zero-shot setups<br/></li> <li><a href="https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html">2020 - wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a><br/></li> </ul> </li> </ul> <p><br/><br/></p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[‚õ∫Ô∏è]]></summary></entry><entry><title type="html">Books and Summary</title><link href="https://yiruyang2025.github.io/blog/2025/Books-and-Summary/" rel="alternate" type="text/html" title="Books and Summary"/><published>2025-04-26T00:00:00+00:00</published><updated>2025-04-26T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Books%20and%20Summary</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Books-and-Summary/"><![CDATA[<p>Hi thereÔºÅ<br/><br/> Pls discuss if any of these pique your interest!<br/><br/><br/></p> <h1 id="1-april-2025">1. April 2025<br/><br/></h1> <p><strong>1.1</strong> <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf">2025, Deepmind - The Era of Experience</a><br/><br/></p> <p><strong>1.2</strong></p> <p><br/><br/><br/><br/></p> <h1 id="2026">2026<br/><br/></h1> <p><strong>1.3</strong> <a href="https://dokumen.pub/the-probabilistic-method-4th-edition-4ed-1119061954-978-1-119-06195-3.html">2016, The Probabilistic Method, 4th Edition</a><br/><br/><br/><br/></p> <h1 id="2-check-list">2. Check List</h1> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Brush Teeth</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Breakfast <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Oxygen and Stretch</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Yogurt, Toast, and Protein</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Ice-cream</li> </ul> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Go to school <br/><br/><br/><br/></li> </ul>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[‚Ä™üèñÔ∏è‚Ä¨]]></summary></entry><entry><title type="html">Generative Models for Science - 27</title><link href="https://yiruyang2025.github.io/blog/2025/Generative-Models-for-Science-27/" rel="alternate" type="text/html" title="Generative Models for Science - 27"/><published>2025-04-26T00:00:00+00:00</published><updated>2025-04-26T00:00:00+00:00</updated><id>https://yiruyang2025.github.io/blog/2025/Generative%20Models%20for%20Science%20-%2027</id><content type="html" xml:base="https://yiruyang2025.github.io/blog/2025/Generative-Models-for-Science-27/"><![CDATA[<p>Welcome!</p> <p>Let‚Äôs take a look at the history of Generative Models for Science<br/><br/><br/><br/></p> <h1 id="1-references">1. References<br/><br/></h1> <p><strong>1.1</strong> 2024 <a href="https://neurips.cc/virtual/2024/tutorial/99531">NeurlPS tutorial - Flow Matching for Generative Modeling</a> <br/><br/></p> <p><strong>1.2</strong> 2023 <a href="https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-033121-110134">Generative Models: An Interdisciplinary Perspective</a><br/><br/></p> <p><strong>1.3</strong> 2021 <a href="https://ieeexplore.ieee.org/abstract/document/9555209">Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models</a><br/><br/><br/><br/></p> <h1 id="2-optical-experiments-simulation-by-diffusion-models">2. Optical Experiments Simulation by Diffusion Models<br/><br/></h1> <p><strong>2.1</strong> 2025 <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/lpor.202401921?casa_token=b1RPYQ5X85cAAAAA%3ArCeurTg2UnYi-G88T2esLaRmKlHEJdMUywn0UROqVcD1xfZ5jpQTUYlLrIVm97A2wojMePyxuO2fcLc">Tunable Optimally-Coded Snapshot Hyperspectral Imaging for Scene Adaptation</a><br/><br/></p> <p><strong>2.2</strong> 2023 <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/788e086c07b8d6fa6b279df56e512312-Abstract-Conference.html">Binarized Spectral Compressive Imaging</a><br/><br/></p> <p><strong>2.3</strong> 2022 <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.html">Mask-Guided Spectral-Wise Transformer for Efficient Hyperspectral Image Reconstruction</a><br/><br/><br/><br/></p> <h1 id="3-technical-history-of-todays-genai-models">3. Technical History of Today‚Äôs GenAI Models<br/><br/><br/><br/></h1> <h1 id="4-a-coding-sample-from-neurlps-2023---binarized-spectral-compressive-imaging">4. A Coding Sample from NeurlPS 2023 - Binarized Spectral Compressive Imaging<br/><br/></h1> <p><strong>4.1 The Github Project</strong> <a href="https://github.com/caiyuanhao1998/BiSCI">A Toolbox for Binarized Spectral Compressive Imaging</a><br/><br/><br/><br/></p>]]></content><author><name></name></author><category term="AI/ML"/><summary type="html"><![CDATA[ü´ê]]></summary></entry></feed>